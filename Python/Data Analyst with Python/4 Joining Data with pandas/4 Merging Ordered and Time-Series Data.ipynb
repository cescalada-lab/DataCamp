{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Ordered and Time- Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500= pd.read_csv('C:/Users/USUARIO/Documents/Universidad/virtual/DataCamp/Data Manipulation with python/2 Joining Data with pandas/datasets/S&P500.csv')\n",
    "gdp= pd.read_csv('C:/Users/USUARIO/Documents/Universidad/virtual/DataCamp/Data Manipulation with python/2 Joining Data with pandas/datasets/WorldBank_GDP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  Returns\n",
      "0  2008   -38.49\n",
      "1  2009    23.45\n",
      "2  2010    12.78\n"
     ]
    }
   ],
   "source": [
    "print(sp500.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   date  returns\n",
      "0  2008   -38.49\n",
      "1  2009    23.45\n",
      "2  2010    12.78\n",
      "3  2011     0.00\n",
      "4  2012    13.41\n",
      "5  2014    11.39\n",
      "6  2015    -0.73\n",
      "7  2016     9.54\n",
      "8  2017    19.42\n",
      "9  2013    29.60\n"
     ]
    }
   ],
   "source": [
    "sp500 = sp500.rename(columns={'Date': 'date', 'Returns': 'returns'})\n",
    "print(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country Name Country Code     Indicator Name  Year           GDP\n",
      "0        China          CHN  GDP (current US$)  2010  6.087160e+12\n",
      "1      Germany          DEU  GDP (current US$)  2010  3.417090e+12\n",
      "2        Japan          JPN  GDP (current US$)  2010  5.700100e+12\n"
     ]
    }
   ],
   "source": [
    "print(gdp.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Country Name Country Code     Indicator Name  Year           GDP\n",
      "3   United States          USA  GDP (current US$)  2010  1.499210e+13\n",
      "7   United States          USA  GDP (current US$)  2011  1.554260e+13\n",
      "11  United States          USA  GDP (current US$)  2012  1.619700e+13\n",
      "15  United States          USA  GDP (current US$)  2012  1.619700e+13\n",
      "19  United States          USA  GDP (current US$)  2013  1.678480e+13\n",
      "23  United States          USA  GDP (current US$)  2014  1.752170e+13\n",
      "27  United States          USA  GDP (current US$)  2015  1.821930e+13\n",
      "31  United States          USA  GDP (current US$)  2016  1.870720e+13\n",
      "35  United States          USA  GDP (current US$)  2017  1.948540e+13\n",
      "39  United States          USA  GDP (current US$)  2018  2.049410e+13\n"
     ]
    }
   ],
   "source": [
    "gdp= gdp[gdp['Country Code']=='USA']\n",
    "print(gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country Code  Year           GDP\n",
      "3           USA  2010  1.499210e+13\n",
      "7           USA  2011  1.554260e+13\n",
      "11          USA  2012  1.619700e+13\n",
      "15          USA  2012  1.619700e+13\n",
      "19          USA  2013  1.678480e+13\n",
      "23          USA  2014  1.752170e+13\n",
      "27          USA  2015  1.821930e+13\n",
      "31          USA  2016  1.870720e+13\n",
      "35          USA  2017  1.948540e+13\n",
      "39          USA  2018  2.049410e+13\n"
     ]
    }
   ],
   "source": [
    "gdp = gdp.drop(['Country Name','Indicator Name'],axis=1)\n",
    "print(gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country code  year           gdp\n",
      "3           USA  2010  1.499210e+13\n",
      "7           USA  2011  1.554260e+13\n",
      "11          USA  2012  1.619700e+13\n",
      "15          USA  2012  1.619700e+13\n",
      "19          USA  2013  1.678480e+13\n",
      "23          USA  2014  1.752170e+13\n",
      "27          USA  2015  1.821930e+13\n",
      "31          USA  2016  1.870720e+13\n",
      "35          USA  2017  1.948540e+13\n",
      "39          USA  2018  2.049410e+13\n"
     ]
    }
   ],
   "source": [
    "gdp = gdp.rename(columns={'Country Code': 'country code', 'Year': 'year', 'Indicator Name': 'indicator name', 'GDP': 'gdp'})\n",
    "print(gdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between GDP and S&P500\n",
    "In this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.\n",
    "\n",
    "``Two tables have been provided for you, named sp500, and gdp. As always, pandas has been imported for you as pd.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions 1/3\n",
    "\n",
    "* Use merge_ordered() to merge gdp and sp500 using a left join on year and date. Save the results as gdp_sp500.\n",
    "* Print gdp_sp500 and look at the returns for the year 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country code  year           gdp    date  returns\n",
      "0          USA  2010  1.499210e+13  2010.0    12.78\n",
      "1          USA  2011  1.554260e+13  2011.0     0.00\n",
      "2          USA  2012  1.619700e+13  2012.0    13.41\n",
      "3          USA  2012  1.619700e+13  2012.0    13.41\n",
      "4          USA  2013  1.678480e+13  2013.0    29.60\n",
      "5          USA  2014  1.752170e+13  2014.0    11.39\n",
      "6          USA  2015  1.821930e+13  2015.0    -0.73\n",
      "7          USA  2016  1.870720e+13  2016.0     9.54\n",
      "8          USA  2017  1.948540e+13  2017.0    19.42\n",
      "9          USA  2018  2.049410e+13     NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "# Use merge_ordered() to merge gdp and sp500 on year and date\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left')\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(gdp_sp500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions 2/3\n",
    "* Use merge_ordered(), again similar to before, to merge gdp and sp500 use the function's ability to interpolate missing data to forward fill the missing value for returns, assigning this table to the variable gdp_sp500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country code  year           gdp  date  returns\n",
      "0          USA  2010  1.499210e+13  2010    12.78\n",
      "1          USA  2011  1.554260e+13  2011     0.00\n",
      "2          USA  2012  1.619700e+13  2012    13.41\n",
      "3          USA  2012  1.619700e+13  2012    13.41\n",
      "4          USA  2013  1.678480e+13  2013    29.60\n",
      "5          USA  2014  1.752170e+13  2014    11.39\n",
      "6          USA  2015  1.821930e+13  2015    -0.73\n",
      "7          USA  2016  1.870720e+13  2016     9.54\n",
      "8          USA  2017  1.948540e+13  2017    19.42\n",
      "9          USA  2018  2.049410e+13  2017    19.42\n"
     ]
    }
   ],
   "source": [
    "# Use merge_ordered() to merge gdp and sp500, interpolate missing value\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left',  fill_method='ffill')\n",
    "\n",
    "\n",
    "# Print gdp_sp500\n",
    "print (gdp_sp500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions 3/3\n",
    "* Subset the gdp_sp500 table, select the gdp and returns columns, and save as gdp_returns.\n",
    "* Print the correlation matrix of the gdp_returns table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              gdp   returns\n",
      "gdp      1.000000  0.212173\n",
      "returns  0.212173  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Use merge_ordered() to merge gdp and sp500, interpolate missing value\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left',  fill_method='ffill')\n",
    "\n",
    "# Subset the gdp and returns columns\n",
    "gdp_returns = gdp_sp500[[\"gdp\",\"returns\"]]\n",
    "\n",
    "# Print gdp_returns correlation\n",
    "print (gdp_returns.corr() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phillips curve using merge_ordered()\n",
    "There is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.\n",
    "\n",
    "You will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.\n",
    "\n",
    "``The tables unemployment and inflation have been loaded for you.``\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use merge_ordered() to merge the inflation and unemployment tables on date with an inner join, and save the results as inflation_unemploy.\n",
    "* Print the inflation_unemploy variable.\n",
    "* Using inflation_unemploy, create a scatter plot with unemployment_rate on the horizontal axis and cpi (inflation) on the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
    "inflation_unemploy = pd.merge_ordered(inflation,unemployment,on='date',how='inner')\n",
    "\n",
    "# Print inflation_unemploy \n",
    "print(inflation_unemploy)\n",
    "\n",
    "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
    "inflation_unemploy.plot('unemployment_rate','cpi',kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge_ordered() caution, multiple columns\n",
    "When using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. The function sorts the merge on columns in the order provided. In this exercise, we will merge GDP and population data from the World Bank for the Australia and Sweden, reversing the order of the merge on columns. The frequency of the series are different, the GDP values are quarterly, and the population is yearly. Use the forward fill feature to fill in the missing data. Depending on the order provided, the fill forward will use unintended data to fill in the missing values.\n",
    "\n",
    "``The tables gdp and pop have been loaded.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions 1/2\n",
    "* Use merge_ordered() on gdp and pop, merging on columns date and country with the fill feature, save to ctry_date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdp and pop on date and country with fill and notice rows 2 and 3\n",
    "ctry_date = pd.merge_ordered(gdp, pop, on=['date','country'], \n",
    "                             fill_method='ffill')\n",
    "\n",
    "# Print ctry_date\n",
    "print(ctry_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions 2/2\n",
    "* Perform the same merge of gdp and pop, but join on country and date (reverse of step 1) with the fill feature, saving this as date_ctry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdp and pop on country and date with fill\n",
    "date_ctry = pd.merge_ordered(gdp, pop, on=['country','date'], \n",
    "                             fill_method='ffill')\n",
    "\n",
    "# Print date_ctry\n",
    "print(date_ctry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "Nice! When you merge on date first, the table is sorted by date then country. When forward fill is applied, Sweden's population value in January is used to fill in the missing values for both Australia and the Sweden for the remainder of the year. This is not what you want. The fill forward is using unintended data to fill in the missing values. However, when you merge on country first, the table is sorted by country then date, so the forward fill is applied appropriately in this situation.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using merge_asof() to study stocks\n",
    "You have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.\n",
    "\n",
    "``The three log files have been loaded for you as tables named jpm, wells, and bac.``\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use merge_asof() to merge jpm (left table) and wells together on the date_time column, where the rows with the nearest times are matched, and with suffixes=('', '_wells'). Save to jpm_wells.\n",
    "* Use merge_asof() to merge jpm_wells (left table) and bac together on the date_time column, where the rows with the closest times are matched, and with suffixes=('_jpm', '_bac'). Save to jpm_wells_bac.\n",
    "* Using price_diffs, create a line plot of the close price of JPM, WFC, and BAC only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm,wells,on='date_time',suffixes=('', '_wells'), direction=\"nearest\")\n",
    "\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac'),direction='nearest')\n",
    "\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using merge_asof() to create dataset\n",
    "The ``merge_asof()`` function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use ``merge_asof()`` to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.\n",
    "\n",
    "``The tables gdp and recession have been loaded for you.``\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Using merge_asof(), merge gdp and recession on date, with gdp as the left table. Save to the variable gdp_recession.\n",
    "* Create a list using a list comprehension and a conditional expression, named is_recession, where for each row if the gdp_recession['econ_status'] value is equal to 'recession' then enter 'r' else 'g'.\n",
    "* Using gdp_recession, plot a bar chart of gdp versus date, setting the color argument equal to is_recession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge_asof() and merge_ordered() differences\n",
    "The ``merge_asof()`` and ``merge_ordered()`` functions are similar in the type of merge they perform and the input arguments they use. In this exercise, think about how the functions are different.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Drag and drop the statement into the appropriate box for either the merge_asof() function, the merge_ordered() function, or both if it applies to both functions.\n",
    "\n",
    "## ANSWERS:\n",
    "\n",
    "**merge_asof()**\n",
    "* has an argument that can be set to 'fordward' to select the first row in the right table whose key column is greater than or equal to the left's.\n",
    "* It can be use to do fuzzy matcheing of dates between tables.\n",
    "* After matching two tables, if there are missing values at the top of the table from the right table, this function can fill them in.\n",
    "\n",
    "**BOTH**\n",
    "* This function can set the suffix for overlapping column names.\n",
    "* This function can se used when working with ordered or time-series data.\n",
    "\n",
    "**merge_ordered()**\n",
    "* It allows for a right join during the merge.\n",
    "* If it cannot match the rows of the tablesexactly,ir can use fordward fill to interpolate the missing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore financials with .query()\n",
    "You have been given a table of financial data from some popular social network companies called social_fin. All of the values are in thousands of US dollars.\n",
    "\n",
    "``Use the .query() method and the IPython shell to explore social_fin and select the True statement.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Answers\n",
    "\n",
    "* There 2 rows where the value is greater than $50,000,000K.\n",
    "\n",
    "``social_fin.query('value>50000000')``\n",
    "``FALSE: there are 3 rows``\n",
    "\n",
    "* There are 3 rows for total revenue for Facebook.\n",
    "\n",
    "``social_fin.query('(financial==\"total_revenue\") and (company==\"facebook\")')``\n",
    "``FALSE: there are 4 rows``\n",
    "\n",
    "* There are 6 rows where the net income has a negative value.\n",
    "\n",
    "``social_fin.query('financial==\"net_income\" and value<0')`` \n",
    "``TRUE: there are 6 rows``\n",
    "\n",
    "* There are 45 rows, where the gross profit is greater than $100K.\n",
    "``social_fin.query('(financial==\"gross_profit\") and (value>100000)')``\n",
    "``FALSE: there are 11 rows``\n",
    "\n",
    "``ANSWER: There are 6 rows where the net income has a negative value.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting rows with .query()\n",
    "In this exercise, you will revisit GDP and population data for Australia and Sweden from the World Bank and expand on it using the .query() method. You'll merge the two tables and compute the GDP per capita. Afterwards, you'll use the .query() method to sub-select the rows and create a plot. Recall that you will need to merge on multiple columns in the proper order.\n",
    "\n",
    "``The tables gdp and pop have been loaded for you.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions 1/4\n",
    "* Use merge_ordered() on gdp and pop on columns country and date with the fill feature, save to gdp_pop and print.\n",
    "\n",
    "## Instructions 2/4\n",
    "* Add a column named gdp_per_capita to gdp_pop that divides gdp by pop.\n",
    "\n",
    "## Instructions 3/4\n",
    "* Pivot gdp_pop so values='gdp_per_capita', index='date', and columns='country', save as gdp_pivot.\n",
    "\n",
    "## Instructions 4/4\n",
    "* Use .query() to select rows from gdp_pivot where date is greater than equal to 1991-01-01\". Save as recent_gdp_pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
    "\n",
    "# Select dates equal to or greater than 1991-01-01\n",
    "recent_gdp_pop = gdp_pivot.query('date >= \"1991-01-01\"')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the right .melt() arguments\n",
    "You are given a table named inflation. Chose the option to get the same output as the table below.\n",
    "````\n",
    "   country    indicator  year  annual\n",
    "0   Brazil  Inflation %  2017    3.45\n",
    "1   Canada  Inflation %  2017    1.60\n",
    "2   France  Inflation %  2017    1.03\n",
    "3    India  Inflation %  2017    2.49\n",
    "4   Brazil  Inflation %  2018    3.66\n",
    "5   Canada  Inflation %  2018    2.27\n",
    "6   France  Inflation %  2018    1.85\n",
    "7    India  Inflation %  2018    4.86\n",
    "8   Brazil  Inflation %  2019    3.73\n",
    "9   Canada  Inflation %  2019    1.95\n",
    "10  France  Inflation %  2019    1.11\n",
    "11   India  Inflation %  2019    7.66\n",
    "````\n",
    "\n",
    "## Possible Answers\n",
    "\n",
    "inflation.melt(id_vars=['country','indicator'], var_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country'], var_name='indicator', value_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country','indicator'], var_name='year', value_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country'], var_name='year', value_name='annual')\n",
    "\n",
    "``ANSWER: inflation.melt(id_vars=['country','indicator'], var_name='year', value_name='annual')``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .melt() to reshape government data\n",
    "The US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.\n",
    "\n",
    "The unemployment rate data has been loaded for you in a table called ur_wide. You are encouraged to view the table in the IPython shell before beginning the exercise.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use .melt() to unpivot all of the columns of ur_wide except year and ensure that the columns with the months and values are named month and unempl_rate, respectively. Save the result as ur_tall.\n",
    "* Add a column to ur_tall named date which combines the year and month columns as year-month format into a larger string, and converts it to a date data type.\n",
    "* Sort ur_tall by date and save as ur_sorted.\n",
    "* Using ur_sorted, plot unempl_rate on the y-axis and date on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpivot everything besides the year column\n",
    "ur_tall = ur_wide.melt(id_vars=['year'],var_name='month',value_name='unempl_rate')\n",
    "\n",
    "\n",
    "# Create a date column using the month and year columns of ur_tall\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['month'] + '-' + ur_tall['year'])\n",
    "\n",
    "# Sort ur_tall by date in ascending order\n",
    "ur_sorted = ur_tall.sort_values('date')\n",
    "\n",
    "# Plot the unempl_rate by date\n",
    "ur_sorted.plot(y='unempl_rate', x='date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .melt() for stocks vs bond performance\n",
    "It is widespread knowledge that the price of bonds is inversely related to the price of stocks. In this last exercise, you'll review many of the topics in this chapter to confirm this. You have been given a table of percent change of the US 10-year treasury bond price. It is in a wide format where there is a separate column for each year. You will need to use the .melt() method to reshape this table.\n",
    "\n",
    "Additionally, you will use the .query() method to filter out unneeded data. You will merge this table with a table of the percent change of the Dow Jones Industrial stock index price. Finally, you will plot data.\n",
    "\n",
    "The tables ten_yr and dji have been loaded for you.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use .melt() on ten_yr to unpivot everything except the metric column, setting var_name='date' and value_name='close'. Save the result to bond_perc.\n",
    "* Using the .query() method, select only those rows were metric equals 'close', and save to bond_perc_close.\n",
    "* Use merge_ordered() to merge dji (left table) and bond_perc_close on date with an inner join, and set suffixes equal to ('_dow', '_bond'). Save the result to dow_bond.\n",
    "* Using dow_bond, plot only the Dow and bond values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use melt on ten_yr, unpivot everything besides the metric column\n",
    "bond_perc = ten_yr.melt(id_vars=['metric'],var_name='date', value_name='close')\n",
    "\n",
    "# Use query on bond_perc to select only the rows where metric=close\n",
    "bond_perc_close = bond_perc.query('metric == \"close\"')\n",
    "\n",
    "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
    "dow_bond = pd.merge_ordered(dji,bond_perc_close, on=\"date\",suffixes=('_dow', '_bond'),how=\"inner\")\n",
    "\n",
    "# Plot only the close_dow and close_bond columns\n",
    "dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d25c592464ae1337381e3c7837c4ce4b4afdd445535cb007d5310b362f92e73a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
