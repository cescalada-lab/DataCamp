{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Classification"
      ],
      "metadata": {
        "id": "D-f48zm5sclf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine learning with scikit-learn\n",
        "\n",
        "### 2. What is machine learning?\n",
        "\n",
        "Machine learning is the process whereby computers learn to make decisions from data without being explicitly programmed.\n",
        "\n",
        "### 3. Examples of machine learning\n",
        "\n",
        "For example, learning to predict whether an email is spam or not spam given its content and sender. Or learning to cluster books into different categories based on the words they contain, then assigning any new book to one of the existing clusters.\n",
        "\n",
        "### 4. Unsupervised learning\n",
        "\n",
        "Unsupervised learning is the process of uncovering hidden patterns and structures from unlabeled data. For example, a business may wish to group its customers into distinct categories based on their purchasing behavior without knowing in advance what these categories are. This is known as clustering, one branch of unsupervised learning.\n",
        "\n",
        "### 5. Supervised learning\n",
        "\n",
        "Supervised learning is a type of machine learning where the values to be predicted are already known, and a model is built with the aim of accurately predicting values of previously unseen data. Supervised learning uses features to predict the value of a target variable, such as predicting a basketball player's position based on their points per game. This course will exclusively focus on supervised learning.\n",
        "\n",
        "### 6. Types of supervised learning\n",
        "\n",
        "There are two types of supervised learning. Classification is used to predict the label, or category, of an observation. For example, we can predict whether a bank transaction is fraudulent or not. As there are two outcomes here - a fraudulent transaction, or non-fraudulent transaction, this is known as binary classification. Regression is used to predict continuous values. For example, a model can use features such as number of bedrooms, and the size of a property, to predict the target variable, price of the property.\n",
        "\n",
        "### 7. Naming conventions\n",
        "\n",
        "Note that what we call a feature throughout the course, others may call a predictor variable or independent variable. Also, what we call the target variable, others may call dependent variable or response variable.\n",
        "\n",
        "### 8. Before you use supervised learning\n",
        "\n",
        "There are some requirements to satisfy before performing supervised learning. Our data must not have missing values, must be in numeric format, and stored as pandas DataFrames or Series, or NumPy arrays. This requires some exploratory data analysis first to ensure data is in the correct format. Various pandas methods for descriptive statistics, along with appropriate data visualizations, are useful in this step.\n",
        "\n",
        "### 9. scikit-learn syntax\n",
        "\n",
        "scikit-learn follows the same syntax for all supervised learning models, which makes the workflow repeatable. Let's familiarize ourselves with the general scikit-learn workflow syntax, before we explore using real data later in the chapter. We import a Model, which is a type of algorithm for our supervised learning problem, from an sklearn module. For example, the k-Nearest Neighbors model uses distance between observations to predict labels or values. We create a variable named model, and instantiate the Model. A model is fit to the data, where it learns patterns about the features and the target variable. We fit the model to X, an array of our features, and y, an array of our target variable values. We then use the model's dot-predict method, passing six new observations, X_new. For example, if feeding features from six emails to a spam classification model, an array of six values is returned. A one indicates the model predicts that email is spam, and a zero represents a prediction of not spam.\n"
      ],
      "metadata": {
        "id": "J67_md6f0jXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The supervised learning workflow\n",
        "Recall that scikit-learn offers a repeatable workflow for using supervised learning models to predict the target variable values when presented with new data.\n",
        "\n",
        "Reorder the pseudo-code provided so it accurately represents the workflow of building a supervised learning model and making predictions."
      ],
      "metadata": {
        "id": "TlD_bpBivNWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image = Image.open('1.png')\n",
        "image.show()"
      ],
      "metadata": {
        "id": "dUGEnY4At3UW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "819f5e83-325c-4305-8aef-060e5cd6b60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=958x382 at 0x7FD784578430>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The classification challenge\n",
        "### 1. The classification challenge\n",
        "\n",
        "Previously, we learned that supervised learning uses labels. Let's discuss how we can build a classification model, or classifier, to predict the labels of unseen data.\n",
        "\n",
        "### 2. Classifying labels of unseen data\n",
        "\n",
        "There are four steps. First, we build a classifier, which learns from the labeled data we pass to it. We then pass it unlabeled data as input, and have it predict labels for this unseen data. As the classifier learns from the labeled data, we call this the training data.\n",
        "\n",
        "### 3. k-Nearest Neighbors\n",
        "\n",
        "Let's build our first model! We'll use an algorithm called k-Nearest Neighbors, which is popular for classification problems. The idea of k-Nearest Neighbors, or KNN, is to predict the label of any data point by looking at the k, for example, three, closest labeled data points and getting them to vote on what label the unlabeled observation should have. KNN uses majority voting, which makes predictions based on what label the majority of nearest neighbors have.\n",
        "\n",
        "Using this scatter plot as an example, how do we classify the black observation?\n",
        "\n",
        "If k equals three, we would classify it as red. This is because two of the three closest observations are red.\n",
        "\n",
        "If k equals five, we would instead classify it as blue.\n",
        "\n",
        "### 7. KNN Intuition\n",
        "\n",
        "To build intuition for KNN, let's look at this scatter plot displaying total evening charge against total day charge for customers of a telecom company. The observations are colored in blue for customers who have churned, and red for those who have not churned.\n",
        "\n",
        "Here we have visualized the results of a KNN algorithm where the number of neighbors is set to 15. KNN creates a decision boundary to predict if customers will churn. Any customers in the area with a gray background are predicted to churn, and those in the area with a red background are predicted to not churn. This boundary would be used to make predictions on unseen data.\n",
        "\n",
        "### 9. Using scikit-learn to fit a classifier\n",
        "\n",
        "To fit a KNN model using scikit-learn, we import KNeighborsClassifier from sklearn-dot-neighbors. We split our data into X, a 2D array of our features, and y, a 1D array of the target values - in this case, churn status. scikit-learn requires that the features are in an array where each column is a feature and each row a different observation. Similarly, the target needs to be a single column with the same number of observations as the feature data. We use the dot-values attribute to convert X and y to NumPy arrays. Printing the shape of X and y, we see there are 3333 observations of two features, and 3333 observations of the target variable. We then instantiate our KNeighborsClassifier, setting n_neighbors equal to 15, and assign it to the variable knn. Then we can fit this classifier to our labeled data by applying the classifier's dot-fit method and passing two arguments: the feature values, X, and the target values, y.\n",
        "\n",
        "### 10. Predicting on unlabeled data\n",
        "\n",
        "Here we have a set of new observations, X_new. Checking the shape of X_new, we see it has three rows and two columns, that is, three observations and two features. We use the classifier's dot-predict method and pass it the unseen data as a 2D NumPy array containing features in columns and observations in rows. Printing the predictions returns a binary value for each observation or row in X_new. It predicts 1, which corresponds to 'churn', for the first observation, and 0, which corresponds to 'no churn', for the second and third observations."
      ],
      "metadata": {
        "id": "zNpLT5mH1iMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring model performance\n",
        "\n",
        "#### 1. Measuring model performance\n",
        "\n",
        "Now we can make predictions using a classifier, but how do we know if the model is making correct predictions? We can evaluate its performance!\n",
        "\n",
        "In classification, accuracy is a commonly-used metric. Accuracy is the number of correct predictions divided by the total number of observations.\n",
        "\n",
        "How do we measure accuracy? We could compute accuracy on the data used to fit the classifier. However, as this data was used to train the model, performance will not be indicative of how well it can generalize to unseen data, which is what we are interested in!\n",
        "\n",
        "#### 4. Computing accuracy\n",
        "\n",
        "It is common to split data into a training set and a test set.\n",
        "\n",
        "We fit the classifier using the training set,\n",
        "\n",
        "then we calculate the model's accuracy against the test set's labels.\n",
        "\n",
        "#### 7. Train/test split\n",
        "\n",
        "To do this, we import train_test_split from sklearn-dot-model_selection. We call train_test_split, passing our features and targets. We commonly use 20-30% of our data as the test set. By setting the test_size argument to zero-point-three we use 30% here. The random_state argument sets a seed for a random number generator that splits the data. Using the same number when repeating this step allows us to reproduce the exact split and our downstream results. It is best practice to ensure our split reflects the proportion of labels in our data. So if churn occurs in 10% of observations, we want 10% of labels in our training and test sets to represent churn. We achieve this by setting stratify equal to y. train_test_split returns four arrays: the training data, the test data, the training labels, and the test labels. We unpack these into X_train, X_test, y_train, and y_test, respectively. We then instantiate a KNN model and fit it to the training data using the dot-fit method. To check the accuracy, we use the dot-score method, passing X test and y test. The accuracy of our model is 88%, which is low given our labels have a 9 to 1 ratio.\n",
        "\n",
        "#### 8. Model complexity\n",
        "\n",
        "Let's discuss how to interpret k. Recall that we discussed decision boundaries, which are thresholds for determining what label a model assigns to an observation. In the image shown, as k increases, the decision boundary is less affected by individual observations, reflecting a simpler model. Simpler models are less able to detect relationships in the dataset, which is known as underfitting. In contrast, complex models can be sensitive to noise in the training data, rather than reflecting general trends. This is known as overfitting.\n",
        "\n",
        "#### 9. Model complexity and over/underfitting\n",
        "\n",
        "We can also interpret k using a model complexity curve. With a KNN model, we can calculate accuracy on the training and test sets using incremental k values, and plot the results. We create empty dictionaries to store our train and test accuracies, and an array containing a range of k values. We use a for loop to repeat our previous workflow, building several models using a different number of neighbors. We loop through our neighbors array and, inside the loop, we instantiate a KNN model with n_neighbors equal to the neighbor iterator, and fit to the training data. We then calculate training and test set accuracy, storing the results in their respective dictionaries.\n",
        "\n",
        "#### 10. Plotting our results\n",
        "\n",
        "After our for loop, we then plot the training and test values, including a legend and labels.\n",
        "\n",
        "#### 11. Model complexity curve\n",
        "\n",
        "Here's the result! As k increases beyond 15 we see overfitting where performance plateaus on both test and training sets, as indicated in this plot.\n",
        "\n",
        "#### 12. Model complexity curve\n",
        "\n",
        "The peak test accuracy actually occurs at around 13 neighbors."
      ],
      "metadata": {
        "id": "PDIii7HMlrwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Regression"
      ],
      "metadata": {
        "id": "XZ7bCe4eZBom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to regression\n",
        "\n",
        "### 1. Introduction to regression\n",
        "\n",
        "Now we're going to check out the other type of supervised learning: regression. In regression tasks, the target variable typically has continuous values, such as a country's GDP, or the price of a house.\n",
        "\n",
        "### 2. Predicting blood glucose levels\n",
        "\n",
        "To conceptualize regression problems, let's use a dataset containing women's health data to predict blood glucose levels. We load the dataset as a pandas DataFrame, and print the first five rows. It contains features including number of pregnancies, triceps skinfold measurements, insulin levels, body mass index, known as BMI, age in years, and diabetes status, with one indicating a diagnosis, and zero representing the absence of a diagnosis.\n",
        "\n",
        "### 3. Creating feature and target arrays\n",
        "\n",
        "Recall that scikit-learn requires features and target values in distinct variables, X and y. To use all of the features in our dataset, we drop our target, blood glucose levels, and store the values attribute as X. For y, we take the the target column's values attribute. We can print the type for X and y to confirm they are now both NumPy arrays.\n",
        "\n",
        "### 4. Making predictions from a single feature\n",
        "\n",
        "To start, let's try to predict blood glucose levels from a single feature: body mass index. To do this, we slice out the BMI column of X, which is the fourth column, storing as the variable X_bmi. Checking the shape of y and X_bmi, we see that they are both one-dimensional arrays. This is fine for y, but our features must be formatted as a two-dimensional array to be accepted by scikit-learn. To convert the shape of X_bmi we apply NumPy's dot-reshape method, passing minus one followed by one. Printing the shape again shows X_bmi is now the correct shape for our model.\n",
        "\n",
        "### 5. Plotting glucose vs. body mass index\n",
        "\n",
        "Now, let's plot blood glucose levels as a function of body mass index. We import matplotlib-dot-pyplot as plt, then pass X_bmi and y to plt-dot-scatter. We'll also label our axes using the xlabel and ylabel methods.\n",
        "\n",
        "### 6. Plotting glucose vs. body mass index\n",
        "\n",
        "We can see that, generally, as body mass index increases, blood glucose levels also tend to increase.\n",
        "\n",
        "### 7. Fitting a regression model\n",
        "\n",
        "It's time to fit a regression model to our data. We're going to use a model called linear regression, which fits a straight line to our data. We will explain the mechanics of linear regression in the next video, but first, let's see how to fit it and plot predictions. We import LinearRegression from sklearn-dot-linear_model, and instantiate our regression model. As we are modeling the relationship between the feature, body mass index, and the target, blood glucose levels, rather than predicting target values for new observations, we fit the model to all of our feature observations. We do this by calling reg-dot-fit and passing in the feature data and the target variable, the same as we did for classification problems. After this, we can create the predictions variable by calling reg-dot-predict and passing in our features. As we are predicting the target values of the features used to train the model, this gives us a line of best fit for our data. We produce our scatter plot again, and then call plt-dot-plot to produce a line plot, passing our features, X_bmi, followed by our predictions.\n",
        "\n",
        "### 8. Fitting a regression model\n",
        "\n",
        "The black line represents the linear regression model's fit of blood glucose values against body mass index, which appears to have a weak-to-moderate positive correlation."
      ],
      "metadata": {
        "id": "cj1wxBD8Y7ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The basics of linear regression\n",
        "\n",
        "### 1. The basics of linear regression\n",
        "\n",
        "So, how does linear regression work?\n",
        "\n",
        "### 2. Regression mechanics\n",
        "\n",
        "We want to fit a line to the data, and in two dimensions this takes the form of y equals ax plus b. Using a single feature is known as simple linear regression, where y is the target, x is the feature, and a and b are the model parameters that we want to learn. a and b are also called the model coefficients, or the slope and intercept, respectively. So how do we accurately choose values for a and b? We can define an error function for any given line and then choose the line that minimizes this function. Error functions are also called loss or cost functions.\n",
        "\n",
        "### 3. The loss function\n",
        "\n",
        "Let's visualize a loss function using this scatter plot. We want the line to be as close to the\n",
        "\n",
        "observations as possible. Therefore, we want to minimize the vertical distance between the fit and the data. So for each observation,\n",
        "\n",
        "we calculate the vertical distance between it and the line.\n",
        "\n",
        "This distance is called a residual. We could try to minimize the sum of the residuals, but then each positive residual would cancel out each negative residual. To avoid this, we square the residuals. By adding all the squared residuals, we calculate the residual sum of squares, or RSS. This type of linear regression is called Ordinary Least Squares, or OLS, where we aim to minimize the RSS.\n",
        "\n",
        "### 9. Linear regression in higher dimensions\n",
        "\n",
        "When we have two features, x1 and x2, and one target, y, a line takes the form y = a1x1 + a2x2 + b. So to fit a linear regression model we specify three variables, a1, a2, and the intercept, b. When adding more features, it is known as multiple linear regression. Fitting a multiple linear regression model means specifying a coefficient, a n, for n number of features, and b. For multiple linear regression models, scikit-learn expects one variable each for feature and target values.\n",
        "\n",
        "### 10. Linear regression using all features\n",
        "\n",
        "Let's perform linear regression to predict blood glucose levels using all of the features from the diabetes dataset. We import LinearRegression from sklearn-dot-linear_model. Then we split the data into training and test sets, instantiate the model, fit it on the training set, and predict on the test set. Note that linear regression in scikit-learn performs OLS under the hood.\n",
        "\n",
        "### 11. R-squared\n",
        "\n",
        "The default metric for linear regression is R-squared, which quantifies the amount of variance in the target variable that is explained by the features. Values can range from zero to one, with one meaning the features completely explain the target's variance. Here are two plots visualizing high and low R-squared respectively.\n",
        "\n",
        "### 12. R-squared in scikit-learn\n",
        "\n",
        "To compute R-squared, we call the model's dot-score method, passing the test features and targets. Here the features only explain about 35 percent of blood glucose level variance.\n",
        "\n",
        "### 13. Mean squared error and root mean squared error\n",
        "\n",
        "Another way to assess a regression model's performance is to take the mean of the residual sum of squares. This is known as the mean squared error, or MSE. MSE is measured in units of our target variable, squared. For example, if a model is predicting a dollar value, MSE will be in dollars squared. To convert to dollars, we can take the square root, known as the root mean squared error, or RMSE.\n",
        "\n",
        "### 14. RMSE in scikit-learn\n",
        "\n",
        "To calculate RMSE, we import mean_squared_error from sklearn-dot-metrics, then call mean_squared_error. We pass y_test and y_pred, and set squared equal to False, which returns the square root of the MSE. The model has an average error for blood glucose levels of around 24 milligrams per deciliter."
      ],
      "metadata": {
        "id": "bVRdug25lK9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation\n",
        "\n",
        "### 1. Cross-validation\n",
        "\n",
        "Great work on those regression challenges! Hopefully we are now feeling familiar with train test split and computing model performance metrics on our test set. But, there is a potential pitfall of this process.\n",
        "\n",
        "### 2. Cross-validation motivation\n",
        "\n",
        "If we're computing R-squared on our test set, the R-squared returned is dependent on the way that we split up the data! The data points in the test set may have some peculiarities that mean the R-squared computed on it is not representative of the model's ability to generalize to unseen data. To combat this dependence on what is essentially a random split, we use a technique called cross-validation.\n",
        "\n",
        "### 3. Cross-validation basics\n",
        "\n",
        "We begin by splitting the dataset into five groups or folds.\n",
        "\n",
        "Then we set aside the first fold as a test set, fit our model on the remaining four folds, predict on our test set, and compute the metric of interest, such as R-squared.\n",
        "\n",
        "Next, we set aside the second fold as our test set, fit on the remaining data, predict on the test set, and compute the metric of interest.\n",
        "\n",
        "Then similarly with the third fold, the fourth fold, and the fifth fold. As a result we get five values of R-squared from which we can compute statistics of interest, such as the mean, median, and 95% confidence intervals.\n",
        "\n",
        "### 13. Cross-validation and model performance\n",
        "\n",
        "As we split the dataset into five folds, we call this process 5-fold cross-validation. If we use 10 folds, it is called 10-fold cross-validation. More generally, if we use k folds, it is called k-fold cross-validation or k-fold CV. There is, however, a trade-off. Using more folds is more computationally expensive. This is because we are fitting and predicting more times.\n",
        "\n",
        "### 14. Cross-validation in scikit-learn\n",
        "\n",
        "To perform k-fold cross-validation in scikit-learn, we import cross_val_score from sklearn-dot-model_selection. We also import KFold, which allows us to set a seed and shuffle our data, making our results repeatable downstream. We first call KFold. The n_splits argument has a default of five, but in this case we assign six, allowing us to use six folds from our dataset for cross-validation. We also set shuffle to True, which shuffles our dataset before splitting into folds. We also assign a seed to the random_state keyword argument, ensuring our data would be split in the same way if we repeat the process making the results repeatable downstream. We save this as the variable kf. As usual, we instantiate our model, in this case, linear regression. We then call cross_val_score, passing the model, the feature data, and the target data as the first three positional arguments. We also specify the number of folds by setting the keyword argument cv equal to our kf variable. This returns an array of cross-validation scores, which we assign to cv_results. The length of the array is the number of folds utilized. Note that the score reported is R squared, as this is the default score for linear regression.\n",
        "\n",
        "### 15. Evaluating cross-validation peformance\n",
        "\n",
        "We can now print the scores. This returns six results ranging from zero-point-seven to approximately zero-point-seven-seven. We can calculate the mean score using np-dot-mean, and the standard deviation using np-dot-std. Additionally, we can calculate the 95% confidence interval using the np-dot-quantile function, passing our results followed by a list containing the upper and lower limits of our interval as decimals."
      ],
      "metadata": {
        "id": "LlT08os_ycQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularized regression \n",
        "\n",
        "### 1. Regularized regression\n",
        "\n",
        "Now let's explore regularization in regression, a technique used to avoid overfitting.\n",
        "\n",
        "### 2. Why regularize?\n",
        "\n",
        "Recall that fitting a linear regression model minimizes a loss function to choose a coefficient, a, for each feature, and the intercept, b. If we allow these coefficients to be very large, we can get overfitting. Therefore, it is common practice to alter the loss function so that it penalizes large coefficients. This is called regularization.\n",
        "\n",
        "### 3. Ridge regression\n",
        "\n",
        "The first type of regularized regression that we'll look at is called ridge. With ridge, we use the Ordinary Least Squares loss function plus the squared value of each coefficient, multiplied by a constant, alpha. So, when minimizing the loss function, models are penalized for coefficients with large positive or negative values. When using ridge, we need to choose the alpha value in order to fit and predict. Essentially, we can select the alpha for which our model performs best. Picking alpha for ridge is similar to picking k in KNN. Alpha in ridge is known as a hyperparameter, which is a variable used for selecting a model's parameters. Alpha controls model complexity. When alpha equals zero, we are performing OLS, where large coefficients are not penalized and overfitting may occur. A high alpha means that large coefficients are significantly penalized, which can lead to underfitting.\n",
        "\n",
        "### 4. Ridge regression in scikit-learn\n",
        "\n",
        "To perform ridge regression in scikit-learn, we import Ridge from sklearn-dot-linear_model. To highlight the impact of different alpha values, we create an empty list for our scores, then loop through a list of different alpha values. Inside the for loop we instantiate Ridge, setting the alpha keyword argument equal to the iterator, also called alpha. We fit on the training data, and predict on the test data. We save the model's R-squared value to the scores list. Finally, outside of the loop, we print the scores for the models with five different alpha values. We see performance gets worse as alpha increases.\n",
        "\n",
        "### 5. Lasso regression\n",
        "\n",
        "There is another type of regularized regression called lasso, where our loss function is the OLS loss function plus the absolute value of each coefficient multiplied by some constant, alpha.\n",
        "\n",
        "### 6. Lasso regression in scikit-learn\n",
        "\n",
        "To use Lasso we import it from sklearn-dot-linear_model. The actual method for performing lasso regression in scikit-learn mirrors ridge regression, as we can see here. Performance drops substantially as alpha goes over 20!\n",
        "\n",
        "### 7. Lasso regression for feature selection\n",
        "\n",
        "Lasso regression can actually be used to assess feature importance. This is because it tends to shrink the coefficients of less important features to zero. The features whose coefficients are not shrunk to zero are selected by the lasso algorithm. Let's check this out in practice.\n",
        "\n",
        "### 8. Lasso for feature selection in scikit-learn\n",
        "\n",
        "We import Lasso. Next, we create our feature and target arrays, and use the dataset's dot-columns attribute to access the feature names and store as the variable names. As we are calculating feature importance we use the entire dataset, rather than splitting it. We then instantiate Lasso, setting alpha to zero-point-one. We fit the model to the data and extract the coefficients using the dot-coef-underscore attribute, storing as lasso_coef. We then plot the coefficients for each feature.\n",
        "\n",
        "### 9. Lasso for feature selection in scikit-learn\n",
        "\n",
        "We can see that the most important predictor for our target variable, blood glucose levels, is the binary value for whether an individual has diabetes or not! This is not surprising, but is a great sanity check. This type of feature selection is very important because it allows us to communicate results to non-technical audiences. It is also useful for identifying which factors are important predictors for various physical phenomena."
      ],
      "metadata": {
        "id": "Rt1GLuMJdV-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Fine-Tuning Your Model"
      ],
      "metadata": {
        "id": "Z4mVdo_qAc3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How good is your model?\n",
        "\n",
        "1. How good is your model?\n",
        "\n",
        "Thinking back to classification problems,\n",
        "\n",
        "2. Classification metrics\n",
        "\n",
        "recall that we can use accuracy, the fraction of correctly classified labels, to measure model performance. However, accuracy is not always a useful metric.\n",
        "\n",
        "3. Class imbalance\n",
        "\n",
        "Consider a model for predicting whether a bank transaction is fraudulent, where only 1% of transactions are actually fraudulent. We could build a model that classifies every transaction as legitimate; this model would have an accuracy of 99%! However, it does a terrible job of actually predicting fraud, so it fails at its original purpose. The situation where one class is more frequent is called class imbalance. Here, the class of legitimate transactions contains way more instances than the class of fraudulent transactions. This is a common situation in practice and requires a different approach to assessing the model's performance.\n",
        "\n",
        "4. Confusion matrix for assessing classification performance\n",
        "\n",
        "Given a binary classifier, such as our fraudulent transactions example, we can create a 2-by-2 matrix that summarizes performance called a confusion matrix.\n",
        "\n",
        "5. Assessing classification performance\n",
        "\n",
        "Across the top are the predicted labels, and down the side are the actual labels.\n",
        "\n",
        "Given any model, we can fill in the confusion matrix according to its predictions.\n",
        "\n",
        "The true positives are the number of fraudulent transactions correctly labeled;\n",
        "\n",
        "The true negatives are the number of legitimate transactions correctly labeled;\n",
        "\n",
        "The false negatives are the number of legitimate transactions incorrectly labeled;\n",
        "\n",
        "And the false positives are the number of transactions incorrectly labeled as fraudulent.\n",
        "\n",
        "Usually, the class of interest is called the positive class. As we aim to detect fraud, the positive class is an illegitimate transaction. So why is the confusion matrix important? Firstly, we can retrieve accuracy: it's the sum of true predictions divided by the total sum of the matrix.\n",
        "\n",
        "13. Precision\n",
        "\n",
        "Secondly, there are other important metrics we can calculate from the confusion matrix. Precision is the number of true positives divided by the sum of all positive predictions. It is also called the positive predictive value. In our case, this is the number of correctly labeled fraudulent transactions divided by the total number of transactions classified as fraudulent. High precision means having a lower false positive rate. For our classifier, this translates to fewer legitimate transactions being classified as fraudulent.\n",
        "\n",
        "14. Recall\n",
        "\n",
        "Recall is the number of true positives divided by the sum of true positives and false negatives. This is also called sensitivity. High recall reflects a lower false negative rate. For our classifier, it means predicting most fraudulent transactions correctly.\n",
        "\n",
        "15. F1 score\n",
        "\n",
        "The F1-score is the harmonic mean of precision and recall. This metric gives equal weight to precision and recall, therefore it factors in both the number of errors made by the model and the type of errors. The F1 score favors models with similar precision and recall, and is a useful metric if we are seeking a model which performs reasonably well across both metrics.\n",
        "\n",
        "16. Confusion matrix in scikit-learn\n",
        "\n",
        "Using our churn dataset, to compute the confusion matrix, along with the metrics, we import classification_report and confusion_matrix from sklearn-dot-metrics. We instantiate our classifier, split the data, fit the training data, and predict the labels of the test set.\n",
        "\n",
        "We pass the test set labels and the predicted labels to the confusion matrix function. We can see 1106 true negatives in the top left.\n",
        "\n",
        "18. Classification report in scikit-learn\n",
        "\n",
        "Passing the same arguments to classification report outputs all the relevant metrics. It includes precision and recall by class, point-seven-six and point-one-six for the churn class respectively, which highlights how poorly the model's recall is on the churn class. Support represents the number of instances for each class within the true labels."
      ],
      "metadata": {
        "id": "mpmR2JEX_92E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deciding on a primary metric\n",
        "As you have seen, several metrics can be useful to evaluate the performance of classification models, including accuracy, precision, recall, and F1-score.\n",
        "\n",
        "In this exercise, you will be provided with three different classification problems, and your task is to select the problem where precision is best suited as the primary metric.\n",
        "\n",
        "Answer the question\n",
        "\n",
        "+ A model predicting if a customer is a high-value lead for a sales team with limited capacity."
      ],
      "metadata": {
        "id": "xNjnuXN8EM_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression and the ROC curve\n",
        "\n",
        "### 2. Logistic regression for binary classification\n",
        "\n",
        "Despite its name, logistic regression is used for classification. This model calculates the probability, p, that an observation belongs to a binary class. Using our diabetes dataset as an example, if p is more than or equal to zero-point-five, we label the data as one, representing a prediction that an individual is more likely to have diabetes; if p is less than zero-point-five, we label it zero to represent that they are more likely to not have diabetes.\n",
        "\n",
        "### 3. Linear decision boundary\n",
        "\n",
        "Note that logistic regression produces a linear decision boundary, as we can see in this image.\n",
        "\n",
        "### 4. Logistic regression in scikit-learn\n",
        "\n",
        "Using logistic regression in scikit-learn follows the same approach as used for other models. We first import LogisticRegression from sklearn-dot-linear_model. Next we instantiate the classifier, split our data, fit the model on our training data, and predict on our test set. In this video we use the churn dataset.\n",
        "\n",
        "### 5. Predicting probabilities\n",
        "\n",
        "We can predict probabilities of each instance belonging to a class by calling logistic regression's predict_proba method and passing the test features. This returns a 2-dimensional array with probabilities for both classes, in this case, that the individual did not churn, or did churn, respectively. We slice the second column, representing the positive class probabilities, and store the results as y_pred_probs. Here we see the model predicts a probability of point-zero-eight-nine that the first observation has churned.\n",
        "\n",
        "### 6. Probability thresholds\n",
        "\n",
        "The default probability threshold for logistic regression in scikit-learn is zero-point-five. This threshold can also apply to other models such as KNN. So what happens as we vary this threshold?\n",
        "\n",
        "### 7. The ROC curve\n",
        "\n",
        "We can use a receiver operating characteristic, or ROC curve, to visualize how different thresholds affect true positive and false positive rates. Here, the dotted line represents a chance model, which randomly guesses labels.\n",
        "\n",
        "When the threshold equals zero, the model predicts one for all observations, meaning it will correctly predict all positive values, and incorrectly predict all negative values.\n",
        "\n",
        "If the threshold equals one, the model predicts zero for all data, which means that both true and false positive rates are zero. If we vary the threshold, we get a series of different false positive and true positive rates.\n",
        "\n",
        "A line plot of the thresholds helps to visualize the trend.\n",
        "\n",
        "### 13. Plotting the ROC curve\n",
        "\n",
        "To plot the ROC curve, we import roc_curve from sklearn-dot-metrics. We then call the function roc_curve; we pass the test labels as the first argument, and the predicted probabilities as the second. We unpack the results into three variables: false positive rate, FPR; true positive rate, TPR; and the thresholds. We can then plot a dotted line from zero to one, along with the FPR and TPR;\n",
        "\n",
        "### 14. Plotting the ROC curve\n",
        "\n",
        "to produce a figure such as this. This looks great, but how do we quantify the model's performance based on this plot?\n",
        "\n",
        "### 15. ROC AUC\n",
        "\n",
        "If we have a model with one for true positive rate and zero for false positive rate, this would be the perfect model. Therefore, we calculate the area under the ROC curve, a metric known as AUC. Scores range from zero to one, with one being ideal. Here, the model scores point-six-seven, which is only 34% better than a model making random guesses.\n",
        "\n",
        "### 16. ROC AUC in scikit-learn\n",
        "\n",
        "We can calculate AUC in scikit-learn by importing roc_auc_score from sklearn-dot-metrics. We call roc_auc_score, passing our test labels and our predicted probabilities, calculated by using the model's predict_proba method on X_test. As expected, we get a score of zero-point-six-seven."
      ],
      "metadata": {
        "id": "c29tbGpE3zZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "1. Hyperparameter tuning\n",
        "\n",
        "Now that we know how to evaluate model performance, let's explore how to optimize our model.\n",
        "\n",
        "Recall that we had to choose a value for alpha in ridge and lasso regression before fitting it. Likewise, before fitting and predicting KNN, we choose n_neighbors. Parameters that we specify before fitting a model, like alpha and n_neighbors, are called hyperparameters. So, a fundamental step for building a successful model:\n",
        "\n",
        "3. Choosing the correct hyperparameters\n",
        "\n",
        "is choosing the correct hyperparameters. We can try lots of different values, fit all of them separately, see how well they perform, and choose the best values! This is called hyperparameter tuning. When fitting different hyperparameter values, we use cross-validation to avoid overfitting the hyperparameters to the test set. We can still split the data, but perform cross-validation on the training set. We withhold the test set and use it for evaluating the tuned model.\n",
        "\n",
        "4. Grid search cross-validation\n",
        "\n",
        "One approach for hyperparameter tuning is called grid search, where we choose a grid of possible hyperparameter values to try. For example, we can search across two hyperparameters for a KNN model - the type of metric and a different number of neighbors. Here we have n neighbors between two and eleven in increments of three, and two metrics: euclidean and manhattan. Therefore, we can create a grid of values like this.\n",
        "\n",
        "We perform k-fold cross-validation for each combination of hyperparameters. The mean scores for each combination are shown here.\n",
        "\n",
        "We then choose hyperparameters that performed best, as shown here.\n",
        "\n",
        "7. GridSearchCV in scikit-learn\n",
        "\n",
        "Let's perform a grid search on a regression model using our sales dataset. We import GridSearchCV from sklearn-dot-model_selection. We instantiate KFold. We then specify the names and values of the hyperparameters we wish to tune as the keys and values of a dictionary, param_grid. As always, we instantiate our model. We then call GridSearchCV and pass it our model, the grid we wish to tune over and set cv equal to kf. This returns a GridSearch object that we can then fit to the training data, and this fit performs the actual cross-validated grid search. We can then print the model's attributes best-params-underscore and best-score-underscore, respectively, to retrieve the hyperparameters that perform the best along with the mean cross-validation score over that fold.\n",
        "\n",
        "8. Limitations and an alternative approach\n",
        "\n",
        "Grid search is great. However, the number of fits is equal to the number of hyperparameters multiplied by the number of values multiplied by the number of folds. Therefore, it doesn't scale well! So, performing 3-fold cross-validation for one hyperparameter with 10 values each means 30 fits, while 10-fold cross-validation on 3 hyperparameters with 10 values each equals 900 fits! However, there is another way.\n",
        "\n",
        "9. RandomizedSearchCV\n",
        "\n",
        "We can perform a random search, which picks random hyperparameter values rather than exhaustively searching through all options. Let's demonstrate this approach. We import RandomizedSearchCV from sklearn-dot-model_selection. We set up KFold and param_grid, and instantiate the model as before. We call RandomizedSearchCV using the same arguments and variables as GridSearchCV. We can optionally set the n_iter argument, which determines the number of hyperparameter values tested. So five-fold cross-validation with n_iter set to two performs 10 fits. Again we can access the best hyperparameters and their score. In this case it is able to find the best hyperparameters from our previous grid search!\n",
        "\n",
        "10. Evaluating on the test set\n",
        "\n",
        "We can evaluate model performance on the test set by passing it to a call of the random search object's dot-score method. It actually performs slightly better than the best score in our grid search!"
      ],
      "metadata": {
        "id": "Rjxkqi-_b28W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Preprocessing and Pipelines¶\n",
        "​"
      ],
      "metadata": {
        "id": "ulR_nxgxI1Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data\n",
        "2. scikit-learn requirements\n",
        "\n",
        "Recall that scikit-learn requires numeric data, with no missing values. All the data that we have used so far has been in this format. However, with real-world data, this will rarely be the case, and instead we need to preprocess our data before we can build models.\n",
        "\n",
        "3. Dealing with categorical features\n",
        "\n",
        "Say we have a dataset containing categorical features, such as color. As these are not numeric, scikit-learn will not accept them and we need to convert them into numeric features. We achieve this by splitting the feature into multiple binary features called dummy variables, one for each category. Zero means the observation was not that category, while one means it was.\n",
        "\n",
        "4. Dummy variables\n",
        "\n",
        "Say we are working with a music dataset that has a genre feature with ten values such as Electronic, Hip-Hop, and Rock.\n",
        "\n",
        "We create binary features for each genre. As each song has one genre, each row will have a 1 in one of the ten columns and zeros in the rest. If a song is not any of the first nine genres, then implicitly, it is a rock song. That means we only need nine features, so we can\n",
        "\n",
        "delete the Rock column. If we do not do this, we are duplicating information, which might be an issue for some models.\n",
        "\n",
        "7. Dealing with categorical features in Python\n",
        "\n",
        "To create dummy variables we can use scikit-learn's OneHotEncoder, or pandas' get_dummies. We will use get_dummies.\n",
        "\n",
        "8. Music dataset\n",
        "\n",
        "We will be working with a music dataset in this chapter, for both classification and regression problems. Initially, we will build a regression model using all features in the dataset to predict song popularity. There is one categorical feature, genre, with ten possible values.\n",
        "\n",
        "9. EDA w/ categorical feature\n",
        "\n",
        "This box plot shows how popularity varies by genre. Let's encode this feature using dummy variables.\n",
        "\n",
        "10. Encoding dummy variables\n",
        "\n",
        "We import pandas, read in the DataFrame, and call pd-dot-get_dummies, passing the categorical column. As we only need to keep nine out of our ten binary features, we can set the drop_first argument to True. Printing the first five rows, we see pandas creates nine new binary features. The first song is Jazz, and the second is Rap, indicated by a 1 in the respective columns. To bring these binary features back into our original DataFrame we can use pd-dot-concat, passing a list containing the music DataFrame and our dummies DataFrame, and setting axis equal to one. Lastly, we can remove the original genre column using df-dot-drop, passing the column, and setting axis equal to one.\n",
        "\n",
        "11. Encoding dummy variables\n",
        "\n",
        "If the DataFrame only has one categorical feature, we can pass the entire DataFrame, thus skipping the step of combining variables. If we don't specify a column, the new DataFrame's binary columns will have the original feature name prefixed, so they will start with genre-underscore - as shown here. Notice the original genre column is automatically dropped. Once we have dummy variables, we can fit models as before.\n",
        "\n",
        "12. Linear regression with dummy variables\n",
        "\n",
        "Using the music_dummies DataFrame, the process for creating training and test sets remains unchanged. To perform cross-validation we then create a KFold object, instantiate a linear regression model, and call cross_val_score. We set scoring equal to neg_mean_squared_error, which returns the negative MSE. This is because scikit-learn's cross-validation metrics presume a higher score is better, so MSE is changed to negative to counteract this. We can calculate the training RMSE by taking the square root and converting to positive, achieved by calling numpy-dot-square-root and passing our scores with a minus sign in front."
      ],
      "metadata": {
        "id": "xoxeNsDkJBMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing data\n",
        "\n",
        "2. Missing data\n",
        "\n",
        "When there is no value for a feature in a particular row, we call it missing data. This can happen because there was no observation or the data might be corrupt. Whatever the reason, we need to deal with it.\n",
        "\n",
        "3. Music dataset\n",
        "\n",
        "Previously we worked with a modified music dataset. Now let's inspect the original version, which contains one thousand rows. We do this by chaining pandas' dot-isna with dot-sum and dot-sort_values. Each feature is missing between 8 and 200 values!\n",
        "\n",
        "4. Dropping missing data\n",
        "\n",
        "A common approach is to remove missing observations accounting for less than 5% of all data. To do this, we use pandas' dot-dropna method, passing a list of columns with less than 5% missing values to the subset argument. If there are missing values in our subset column, the entire row is removed. Rechecking the DataFrame, we see fewer missing values.\n",
        "\n",
        "5. Imputing values\n",
        "\n",
        "Another option is to impute missing data. This means making an educated guess as to what the missing values could be. We can impute the mean of all non-missing entries for a given feature. We can also use other values like the median. For categorical values we commonly impute the most frequent value. Note we must split our data before imputing to avoid leaking test set information to our model, a concept known as data leakage.\n",
        "\n",
        "6. Imputation with scikit-learn\n",
        "\n",
        "Here is a workflow for imputation to predict song popularity. We import SimpleImputer from sklearn-dot-impute. As we will use different imputation methods for categorical and numeric features, we first split them, storing as X_cat and X_num respectively, along with our target array as y. We create categorical training and test sets. We repeat this for the numeric features. By using the same value for the random_state argument, the target arrays' values remain unchanged. To impute missing categorical values we instantiate a SimpleImputer, setting strategy as most frequent. By default, SimpleImputer expects NumPy-dot-NaN to represent missing values. Now we call dot-fit_transform to impute the training categorical features' missing values! For the test categorical features, we call dot-transform.\n",
        "\n",
        "7. Imputation with scikit-learn\n",
        "\n",
        "For our numeric data, we instantiate another imputer. By default, it fills values with the mean. We fit and transform the training features, and transform the test features. We then combine our training data using numpy-dot-append, passing our two arrays, and set axis equal to 1. We repeat this for our test data. Due to their ability to transform our data, imputers are known as transformers.\n",
        "\n",
        "8. Imputing within a pipeline\n",
        "\n",
        "We can also impute using a pipeline, which is an object used to run a series of transformations and build a model in a single workflow. To do this, we import Pipeline from sklearn-dot-pipeline. Here we perform binary classification to predict whether a song is rock or another genre. We drop missing values accounting for less than five percent of our data. We convert values in the genre column, which will be the target, to a 1 if Rock, else 0, using numpy-dot-where. We then create X and y.\n",
        "\n",
        "9. Imputing within a pipeline\n",
        "\n",
        "To build a pipeline we construct a list of steps containing tuples with the step names specified as strings, and instantiate the transformer or model. We pass this list when instantiating a Pipeline. We then split our data, and fit the pipeline to the training data, as with any other model. Finally, we compute accuracy. Note that, in a pipeline, each step but the last must be a transformer."
      ],
      "metadata": {
        "id": "mtSsRJBEJXAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centering and scaling\n",
        "\n",
        "1. Centering and scaling\n",
        "\n",
        "Data imputation is one of several important preprocessing steps for machine learning. Now let's cover another: centering and scaling our data.\n",
        "\n",
        "2. Why scale our data?\n",
        "\n",
        "Let's use df-dot-describe to check out the ranges of some of our feature variables in the music dataset. We see that the ranges vary widely: duration_ms ranges from zero to one-point-six-two million, speechiness contains only decimal places, and loudness only has negative values!\n",
        "\n",
        "3. Why scale our data?\n",
        "\n",
        "Many machine learning models use some form of distance to inform them, so if we have features on far larger scales, they can disproportionately influence our model. For example, KNN uses distance explicitly when making predictions. For this reason, we actually want features to be on a similar scale. To achieve this, we can normalize or standardize our data, often referred to as scaling and centering.\n",
        "\n",
        "4. How to scale our data\n",
        "\n",
        "There are several ways to scale our data: given any column, we can subtract the mean and divide by the variance so that all features are centered around zero and have a variance of one. This is called standardization. We can also subtract the minimum and divide by the range of the data so the normalized dataset has minimum zero and maximum one. Or, we can center our data so that it ranges from -1 to 1 instead. In this video, we will perform standardization, but scikit-learn has functions available for other types of scaling.\n",
        "\n",
        "5. Scaling in scikit-learn\n",
        "\n",
        "To scale our features, we import StandardScaler from sklearn-dot-preprocessing. We create our feature and target arrays. Before scaling, we split our data to avoid data leakage. We then instantiate a StandardScaler object, and call its fit_transform method, passing our training features. Next, we use scaler-dot-transform on the test features. Looking at the mean and standard deviation of the columns of both the original and scaled data verifies the change has taken place.\n",
        "\n",
        "6. Scaling in a pipeline\n",
        "\n",
        "We can also put a scaler in a pipeline! Here we build a pipeline object to scale our data and use a KNN model with six neighbors. We then split our data, fit the pipeline to our training set, and predict on our test set. Computing the accuracy yields a result of zero-point-eight-one. Let's compare this to using unscaled data.\n",
        "\n",
        "7. Comparing performance using unscaled data\n",
        "02:42 - 02:57\n",
        "Here we fit a KNN model to our unscaled training data and print the accuracy. It is only zero-point-five-three, so just by scaling our data we improved accuracy by over 50 percent!\n",
        "\n",
        "8. CV and scaling in a pipeline\n",
        "\n",
        "Let's also look at how we can use cross-validation with a pipeline. We first build our pipeline. We then specify our hyperparameter space by creating a dictionary: the keys are the pipeline step name followed by a double underscore, followed by the hyperparameter name. The corresponding value is a list or an array of the values to try for that particular hyperparameter. In this case, we are tuning n_neighbors in the KNN model. Next we split our data into training and test sets. We then perform a grid search over our parameters by instantiating the GridSearchCV object, passing our pipeline and setting the param_grid argument equal to parameters. We then fit it to our training data. Lastly, we make predictions using our test set.\n",
        "\n",
        "9. Checking model parameters\n",
        "\n",
        "Printing GridSearchCV's best_score_ attribute, we see the score is very slightly better than our previous model's performance. Printing the best parameters, the optimal model has 12 neighbors."
      ],
      "metadata": {
        "id": "Rgz0mndWJl6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating multiple models\n",
        "\n",
        "1. Evaluating multiple models\n",
        "\n",
        "We've covered all parts of the supervised learning workflow. But how do we decide which model to use in the first place?\n",
        "\n",
        "2. Different models for different problems\n",
        "\n",
        "This is a complex question, and the answer depends on our situation. However, there are some principles that can guide us when making this decision. The size of our dataset plays a role. Fewer features means a simpler model, and can reduce training time. Also, some models, such as Artificial Neural Networks, require a lot of data to perform well. We may need an interpretable model, so we can explain to stakeholders how predictions were made. An example is linear regression, where we can calculate and interpret the model coefficients. Alternatively, flexibility might be important to get the most accurate predictions. Generally, flexible models make fewer assumptions about the data; for example, a KNN model does not assume a linear relationship between the features and the target.\n",
        "\n",
        "3. It's all in the metrics\n",
        "\n",
        "Notice that scikit-learn allows the same methods to be used for most models. This makes it easy to compare them! Regression models can be evaluated using the root mean squared error, or the R-squared value. Likewise, classification models can all be analyzed using accuracy, a confusion matrix and its associated metrics, or the ROC AUC. Therefore, one approach is to select several models and a metric, then evaluate their performance without any form of hyperparameter tuning.\n",
        "\n",
        "4. A note on scaling\n",
        "\n",
        "Recall that the performance of some models, such as KNN, linear regression, and logistic regression, are affected by scaling our data. Therefore, it is generally best to scale our data before evaluating models out of the box.\n",
        "\n",
        "5. Evaluating classification models\n",
        "\n",
        "We will evaluate three models for binary classification of song genre: KNN, logistic regression, and a new model called a decision tree classifier. We import our required modules, including DecisionTreeClassifier from sklearn-dot-tree. The workings of decision trees are outside the scope of this course, but the steps for building this model are the same as for other models in scikit-learn. As usual, we create our feature and target arrays, then split our data. We then scale our features using the scaler's dot-fit_transform method on the training set, and the dot-transform method on the test set.\n",
        "\n",
        "6. Evaluating classification models\n",
        "\n",
        "We create a dictionary with our model names as strings for the keys, and instantiate models as the dictionary's values. We also create an empty list to store the results. Now we loop through the models in our models dictionary, using its dot-values method. Inside the loop, we instantiate a KFold object. Next we perform cross-validation, using the model being iterated, along with our scaled training features, and target training array. We set cv equal to our kfold variable. By default, the scoring here will be accuracy. We then append the cross-validation results to our results list. Lastly, outside of the loop, we create a boxplot of our results, and set the labels argument equal to a call of models-dot-keys to retrieve each model's name.\n",
        "\n",
        "7. Visualizing results\n",
        "\n",
        "The output shows us the range of cross-validation accuracy scores. We can also see each model's median cross-validation score, represented by the orange line in each box. We can see logistic regression has the best median score.\n",
        "\n",
        "8. Test set performance\n",
        "\n",
        "To evaluate on the test set we loop through the names and values of the dictionary using the dot-items method. Inside the loop we fit the model, calculate accuracy, and print it. Logistic regression performs best for this problem if we are using accuracy as the metric."
      ],
      "metadata": {
        "id": "67_B6voIKC83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCMuDP9cKMnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}